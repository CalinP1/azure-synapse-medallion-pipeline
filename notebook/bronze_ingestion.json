{
	"name": "bronze_ingestion",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "c9096cf7-2d18-4427-ba66-2728e3a2d26f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/32ed6f2a-a561-477a-8242-a37e60600b34/resourceGroups/synapseWork/providers/Microsoft.Synapse/workspaces/synapseforwork/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://synapseforwork.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Importing the libraries needed. Pyspark to work with data, requests for retrieveing data from kaggle, zipfile to unzip the retrieved data, io to transform the binary data to a file-like object.**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"import requests\n",
					"import zipfile\n",
					"import io\n",
					"from datetime import datetime\n",
					"display(\"Libs imported succesfully\")\n",
					"display(f\"Check the time - {datetime.now()}\")"
				],
				"execution_count": 65
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"We import Microsoft spark utilities to retrieve the secret name and key from vault for the Kaggle api fetch (this method is specific only for Azure Synapse)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from notebookutils import mssparkutils"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"source": [
					"vault_url = \"https://kaggle-key-api.vault.azure.net/\"\n",
					"vault_name = \"kaggle-name\"\n",
					"vault_key = 'kaggle-key'"
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"source": [
					"vault_kaggle_name = mssparkutils.credentials.getSecret(vault_url, vault_name)\n",
					"vault_kaggle_key = mssparkutils.credentials.getSecret(vault_url, vault_key)"
				],
				"execution_count": 68
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"We check to see if we did the right credentials retrieval"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"display(f\"Username type: {type(vault_kaggle_name)}\")\n",
					"display(f\"Username length: {len(vault_kaggle_name)} characters\")"
				],
				"execution_count": 69
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Kaggle API call"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dataset_name = \"olistbr/brazilian-ecommerce\"\n",
					"kaggle_dataset_url = f\"https://www.kaggle.com/api/v1/datasets/download/{dataset_name}\""
				],
				"execution_count": 70
			},
			{
				"cell_type": "code",
				"source": [
					"kaggle_response = requests.get(kaggle_dataset_url,auth=(vault_kaggle_name,vault_kaggle_key))\n",
					"if kaggle_response.status_code == 200:\n",
					"    display(\"SUCCESS!!\")\n",
					"    size_mb = len(kaggle_response.content) / 1024 / 1024\n",
					"    display(f\"Size: {size_mb:.2f} MB\")\n",
					"\n",
					"    zip_file = zipfile.ZipFile(io.BytesIO(kaggle_response.content))\n",
					"    file_list = zip_file.namelist()\n",
					"    display(f\"Files in the datase: {len(file_list)}\")\n",
					"    for file in file_list:\n",
					"        display(file)\n",
					"else:\n",
					"    display(f\"We have a problem... CODE:{kaggle_response.status_code}\")"
				],
				"execution_count": 71
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Loading the data from the csv file inside parquet in spark"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account = \"forsynapsework\"\n",
					"container = \"bronze\"\n",
					"base_folder = \"raw/ecommerce\""
				],
				"execution_count": 72
			},
			{
				"cell_type": "code",
				"source": [
					"bronze_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{base_folder}\""
				],
				"execution_count": 73
			},
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd"
				],
				"execution_count": 77
			},
			{
				"cell_type": "code",
				"source": [
					"for file_name in file_list:\n",
					"    \n",
					"    if not file_name.endswith('.csv'):\n",
					"        continue\n",
					"    \n",
					"    display(f\"Processing: {file_name}\")\n",
					"    \n",
					"    csv_data = zip_file.read(file_name)\n",
					"    pandas_df = pd.read_csv(io.BytesIO(csv_data))\n",
					"    \n",
					"    display(f\"Rows: {len(pandas_df)}\")\n",
					"    display(f\"Columns: {len(pandas_df.columns)}\")\n",
					"\n",
					"    spark_df = spark.createDataFrame(pandas_df)\n",
					"    dataset_name = file_name.replace('.csv', '')\n",
					"    output_path = f\"{bronze_path}/{dataset_name}\"\n",
					"    \n",
					"    spark_df.write.mode(\"overwrite\").parquet(output_path)\n",
					"    \n",
					"    display(f\"Written to: {dataset_name}\")\n",
					"    \n",
					"display(\"Done all!\")"
				],
				"execution_count": 79
			}
		]
	}
}