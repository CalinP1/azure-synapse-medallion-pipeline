{
	"name": "silver_transform",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "523663a5-3b14-4710-afcf-4c4d684743e2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/32ed6f2a-a561-477a-8242-a37e60600b34/resourceGroups/synapseWork/providers/Microsoft.Synapse/workspaces/synapseforwork/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://synapseforwork.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.types import *\n",
					"from datetime import datetime"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account = \"forsynapsework\"\n",
					"bronze_container = \"bronze\"\n",
					"silver_container = \"silver\"\n",
					"\n",
					"bronze_path = f\"abfss://{bronze_container}@{storage_account}.dfs.core.windows.net/raw/ecommerce\"\n",
					"silver_path = f\"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/cleaned\"\n",
					""
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"display(f\"Bronze: {bronze_path}\")\n",
					"display(f\"Silver: {silver_path}\")\n",
					"display(f\"Started: {datetime.now()}\")\n",
					"\n",
					"\n",
					"bronze_datasets = mssparkutils.fs.ls(bronze_path)\n",
					"display(f\"Found {len(bronze_datasets)} datasets Ã®n Bronze\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"We are creating a dictionary that will sore key-value with the table name and it's reference"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dfs = {}\n",
					"\n",
					"for dataset in bronze_datasets:\n",
					"    dataset_name = dataset.name.replace(\"/\",\"\")\n",
					"    df = spark.read.parquet(dataset.path)\n",
					"    dfs[dataset_name] = df\n",
					"    display(f\"Name: {dataset_name}\")\n",
					"    display(f\"Rows: {df.count()} \")\n",
					"    display(f\"Columns:{len(df.columns)}\")\n",
					"\n",
					"display(f\"Done! Available datasets: {list(dfs.keys())}\")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Creating a quality report that will indicate how many missing values and duplicate values we have in each dataset"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					},
					"collapsed": false
				},
				"source": [
					"quality_report = {}\n",
					"\n",
					"for df_name, df in dfs.items():\n",
					"\n",
					"    #for checking null values in each column\n",
					"\n",
					"    null_columns = df.select([F.count(F.when(F.isnull(c),c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
					"    columns_with_nulls = {col:count for col, count in null_columns.items() if count > 0}\n",
					"\n",
					"    if columns_with_nulls:\n",
					"        display(f\"Nulls found:\")\n",
					"        for col, count in columns_with_nulls.items():\n",
					"            display(f\"{col}: {count} nulls\")\n",
					"    else:\n",
					"        display(f\"No nulls!\")\n",
					"    \n",
					"\n",
					"    #for duplicates\n",
					"    total_rows = df.count()\n",
					"    distinct_rows = df.distinct().count()\n",
					"    duplicates = total_rows - distinct_rows\n",
					"\n",
					"    if duplicates > 0:\n",
					"        display(f\"For '{df_name}' we have {duplicates} numbers of duplicates\")\n",
					"    else:\n",
					"        display(\"No duplicates in this dataset!\")\n",
					"\n",
					"\n",
					"\n",
					"    #creating the quaility report dictionary\n",
					"\n",
					"    quality_report[df_name] = {\n",
					"        'total_rows':total_rows,\n",
					"        'total_columns':len(df.columns),\n",
					"        'duplicates': duplicates,\n",
					"        'null_values':columns_with_nulls,\n",
					"    }\n",
					"    "
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"quality_report"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Cleaning Data process"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"cleaned_data = {}\n",
					"\n",
					"for dataset_name, df in dfs.items():\n",
					"    display(f\"START! Now working on: {dataset_name}\")\n",
					"\n",
					"    # duplicates\n",
					"    original_count = df.count()\n",
					"    df_clean = df.dropDuplicates()\n",
					"\n",
					"    # null values\n",
					"    df_clean = df_clean.dropna(how=\"all\")\n",
					"\n",
					"    df_clean = df_clean.withColumn(\"processed_date\", F.current_date()) \\\n",
					"                       .withColumn(\"processed_timestamp\", F.current_timestamp()) \\\n",
					"                       .withColumn(\"source_layer\", F.lit(\"bronze\"))\n",
					"\n",
					"    cleaned_data[dataset_name] = df_clean\n",
					"\n",
					"    new_count = df_clean.count()\n",
					"    removed_data = original_count - new_count\n",
					"    display(f\"We have removed {removed_data} duplicate rows\")\n",
					"\n",
					"    display(f\"DONE! Raw dataset {original_count} - Cleaned dataset {new_count}\")\n",
					"\n",
					""
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Writing data to the silver container"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"for dataset_name, df_clean in cleaned_data.items():\n",
					"    display(f\"START! For {dataset_name}\")\n",
					"    output_path = f\"{silver_path}/{dataset_name}\"\n",
					"    df_clean.write.mode(\"overwrite\").parquet(output_path)\n",
					"\n",
					"    display(f\"DONE! For {dataset_name} we have writen in the {output_path} - {df_clean.count()} rows\")"
				],
				"execution_count": 19
			}
		]
	}
}