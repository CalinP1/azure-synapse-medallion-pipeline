{
	"name": "silver_transform",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c25baa94-b570-4f48-bee6-a7a95b13d21f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/32ed6f2a-a561-477a-8242-a37e60600b34/resourceGroups/synapseWork/providers/Microsoft.Synapse/workspaces/synapseforwork/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://synapseforwork.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.types import *\n",
					"from datetime import datetime"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account = \"forsynapsework\"\n",
					"bronze_container = \"bronze\"\n",
					"silver_container = \"silver\"\n",
					"\n",
					"bronze_path = f\"abfss://{bronze_container}@{storage_account}.dfs.core.windows.net/raw/ecommerce\"\n",
					"silver_path = f\"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/cleaned\"\n",
					""
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"display(f\"Bronze: {bronze_path}\")\n",
					"display(f\"Silver: {silver_path}\")\n",
					"display(f\"Started: {datetime.now()}\")\n",
					"\n",
					"\n",
					"bronze_datasets = mssparkutils.fs.ls(bronze_path)\n",
					"display(f\"Found {len(bronze_datasets)} datasets Ã®n Bronze\")"
				],
				"execution_count": 29
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"We are creating a dictionary that will sore key-value with the table name and it's reference"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dfs = {}\n",
					"\n",
					"for dataset in bronze_datasets:\n",
					"    dataset_name = dataset.name.replace(\"/\",\"\")\n",
					"    df = spark.read.parquet(dataset.path)\n",
					"    dfs[dataset_name] = df\n",
					"    display(f\"Name: {dataset_name}\")\n",
					"    display(f\"Rows: {df.count()} \")\n",
					"    display(f\"Columns:{len(df.columns)}\")\n",
					"\n",
					"display(f\"Done! Available datasets: {list(dfs.keys())}\")"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Creating a quality report that will indicate how many missing values and duplicate values we have in each dataset"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"quality_report = {}\n",
					"\n",
					"for df_name, df in dfs.items():\n",
					"\n",
					"    #for checking null values in each column\n",
					"\n",
					"    null_columns = df.select([F.count(F.when(F.isnull(c),c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
					"    columns_with_nulls = {col:count for col, count in null_columns.items() if count > 0}\n",
					"\n",
					"    if columns_with_nulls:\n",
					"        display(f\"Nulls found:\")\n",
					"        for col, count in columns_with_nulls.items():\n",
					"            display(f\"{col}: {count} nulls\")\n",
					"    else:\n",
					"        display(f\"No nulls!\")\n",
					"    \n",
					"\n",
					"    #for duplicates\n",
					"    total_rows = df.count()\n",
					"    distinct_rows = df.distinct().count()\n",
					"    duplicates = total_rows - distinct_rows\n",
					"\n",
					"    if duplicates > 0:\n",
					"        display(f\"For '{df_name}' we have {duplicates} numbers of duplicates\")\n",
					"    else:\n",
					"        display(\"No duplicates in this dataset!\")\n",
					"\n",
					"\n",
					"\n",
					"    #creating the quaility report dictionary\n",
					"\n",
					"    quality_report[df_name] = {\n",
					"        'total_rows':total_rows,\n",
					"        'total_columns':len(df.columns),\n",
					"        'duplicates': duplicates,\n",
					"        'null_values':columns_with_nulls,\n",
					"    }\n",
					"    "
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"quality_report"
				],
				"execution_count": 38
			}
		]
	}
}