{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapseforwork"
		},
		"synapseforwork-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapseforwork-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapseforwork.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synapseforwork-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://forsynapsework.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/synapseforwork-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapseforwork-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapseforwork-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapseforwork-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bronze_ingestion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "5cb50809-5274-46dc-b0d7-6f822e17df49"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/32ed6f2a-a561-477a-8242-a37e60600b34/resourceGroups/synapseWork/providers/Microsoft.Synapse/workspaces/synapseforwork/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapseforwork.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Importing the libraries needed. Pyspark to work with data, requests for retrieveing data from kaggle, zipfile to unzip the retrieved data, io to transform the binary data to a file-like object.**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"import requests\n",
							"import zipfile\n",
							"import io\n",
							"from datetime import datetime\n",
							"display(\"Libs imported succesfully\")\n",
							"display(f\"Check the time - {datetime.now()}\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We import Microsoft spark utilities to retrieve the secret name and key from vault for the Kaggle api fetch (this method is specific only for Azure Synapse)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"vault_url = \"https://kaggle-key-api.vault.azure.net/\"\n",
							"vault_name = \"kaggle-name\"\n",
							"vault_key = 'kaggle-key'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"vault_kaggle_name = mssparkutils.credentials.getSecret(vault_url, vault_name)\n",
							"vault_kaggle_key = mssparkutils.credentials.getSecret(vault_url, vault_key)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We check to see if we did the right credentials retrieval"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false
							}
						},
						"source": [
							"display(f\"Username type: {type(vault_kaggle_name)}\")\n",
							"display(f\"Username length: {len(vault_kaggle_name)} characters\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Kaggle API call"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"dataset_name = \"olistbr/brazilian-ecommerce\"\n",
							"kaggle_dataset_url = f\"https://www.kaggle.com/api/v1/datasets/download/{dataset_name}\""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"kaggle_response = requests.get(kaggle_dataset_url,auth=(vault_kaggle_name,vault_kaggle_key))\n",
							"if kaggle_response.status_code == 200:\n",
							"    display(\"SUCCESS!!\")\n",
							"    size_mb = len(kaggle_response.content) / 1024 / 1024\n",
							"    display(f\"Size: {size_mb:.2f} MB\")\n",
							"\n",
							"    zip_file = zipfile.ZipFile(io.BytesIO(kaggle_response.content))\n",
							"    file_list = zip_file.namelist()\n",
							"    display(f\"Files in the datase: {len(file_list)}\")\n",
							"    for file in file_list:\n",
							"        display(file)\n",
							"else:\n",
							"    display(f\"We have a problem... CODE:{kaggle_response.status_code}\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Loading the data from the csv file inside parquet in spark"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"storage_account = \"forsynapsework\"\n",
							"container = \"bronze\"\n",
							"base_folder = \"raw/ecommerce\""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"bronze_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{base_folder}\""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import pandas as pd"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"for file_name in file_list:\n",
							"    \n",
							"    if not file_name.endswith('.csv'):\n",
							"        continue\n",
							"    \n",
							"    display(f\"Processing: {file_name}\")\n",
							"    \n",
							"    csv_data = zip_file.read(file_name)\n",
							"    pandas_df = pd.read_csv(io.BytesIO(csv_data))\n",
							"    \n",
							"    display(f\"Rows: {len(pandas_df)}\")\n",
							"    display(f\"Columns: {len(pandas_df.columns)}\")\n",
							"\n",
							"    spark_df = spark.createDataFrame(pandas_df)\n",
							"    dataset_name = file_name.replace('.csv', '')\n",
							"    output_path = f\"{bronze_path}/{dataset_name}\"\n",
							"    \n",
							"    spark_df.write.mode(\"overwrite\").parquet(output_path)\n",
							"    \n",
							"    display(f\"Written to: {dataset_name}\")\n",
							"    \n",
							"display(\"Done all!\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"display(\"Verification for Bronze layer\")\n",
							"\n",
							"bronze_datasets = mssparkutils.fs.ls(bronze_path)\n",
							"\n",
							"display(f\"Total datasets: {len(bronze_datasets)}\")\n",
							"\n",
							"for dataset in bronze_datasets:\n",
							"    display(f\"{dataset.name}\")\n",
							"    \n",
							"    sample_df = spark.read.parquet(dataset.path)\n",
							"    display(f\"Rows: {sample_df.count()}\")\n",
							"    display(f\"Columns: {len(sample_df.columns)}\")\n",
							"    display(f\"Sample columns: {sample_df.columns[:5]}\")\n",
							"\n",
							"display(\"Bronze layer verification complete!\")"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.5",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "northeurope"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/silver_transform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "545c23ba-1cd2-4e56-8d2e-589324bf9f32"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/32ed6f2a-a561-477a-8242-a37e60600b34/resourceGroups/synapseWork/providers/Microsoft.Synapse/workspaces/synapseforwork/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapseforwork.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.types import *\n",
							"from datetime import datetime"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"storage_account = \"forsynapsework\"\n",
							"bronze_container = \"bronze\"\n",
							"silver_container = \"silver\"\n",
							"\n",
							"bronze_path = f\"abfss://{bronze_container}@{storage_account}.dfs.core.windows.net/raw/ecommerce\"\n",
							"silver_path = f\"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/cleaned\"\n",
							""
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"display(f\"Bronze: {bronze_path}\")\n",
							"display(f\"Silver: {silver_path}\")\n",
							"display(f\"Started: {datetime.now()}\")\n",
							"\n",
							"\n",
							"bronze_datasets = mssparkutils.fs.ls(bronze_path)\n",
							"display(f\"Found {len(bronze_datasets)} datasets Ã®n Bronze\")"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We are creating a dictionary that will sore key-value with the table name and it's reference"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"dfs = {}\n",
							"\n",
							"for dataset in bronze_datasets:\n",
							"    dataset_name = dataset.name.replace(\"/\",\"\")\n",
							"    df = spark.read.parquet(dataset.path)\n",
							"    dfs[dataset_name] = df\n",
							"    display(f\"Name: {dataset_name}\")\n",
							"    display(f\"Rows: {df.count()} \")\n",
							"    display(f\"Columns:{len(df.columns)}\")\n",
							"\n",
							"display(f\"Done! Available datasets: {list(dfs.keys())}\")"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Creating a quality report that will indicate how many missing values and duplicate values we have in each dataset"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"quality_report = {}\n",
							"\n",
							"for df_name, df in dfs.items():\n",
							"\n",
							"    #for checking null values in each column\n",
							"\n",
							"    null_columns = df.select([F.count(F.when(F.isnull(c),c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
							"    columns_with_nulls = {col:count for col, count in null_columns.items() if count > 0}\n",
							"\n",
							"    if columns_with_nulls:\n",
							"        display(f\"Nulls found:\")\n",
							"        for col, count in columns_with_nulls.items():\n",
							"            display(f\"{col}: {count} nulls\")\n",
							"    else:\n",
							"        display(f\"No nulls!\")\n",
							"    \n",
							"\n",
							"    #for duplicates\n",
							"    total_rows = df.count()\n",
							"    distinct_rows = df.distinct().count()\n",
							"    duplicates = total_rows - distinct_rows\n",
							"\n",
							"    if duplicates > 0:\n",
							"        display(f\"For '{df_name}' we have {duplicates} numbers of duplicates\")\n",
							"    else:\n",
							"        display(\"No duplicates in this dataset!\")\n",
							"\n",
							"\n",
							"\n",
							"    #creating the quaility report dictionary\n",
							"\n",
							"    quality_report[df_name] = {\n",
							"        'total_rows':total_rows,\n",
							"        'total_columns':len(df.columns),\n",
							"        'duplicates': duplicates,\n",
							"        'null_values':columns_with_nulls,\n",
							"    }\n",
							"    "
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"quality_report"
						],
						"outputs": [],
						"execution_count": 38
					}
				]
			},
			"dependsOn": []
		}
	]
}